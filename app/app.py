import streamlit as st
from transformers import AutoTokenizer, AutoModelForSequenceClassification  # hanya untuk model deteksi
import torch
from langdetect import detect
import re
import nltk
from sumy.parsers.plaintext import PlaintextParser
from sumy.nlp.tokenizers import Tokenizer as SumyTokenizer
from sumy.summarizers.lex_rank import LexRankSummarizer

# --- Streamlit Config ---
st.set_page_config(page_title="Deteksi Berita Hoaks", layout="centered")

# --- NLTK Download ---
try:
    nltk.data.find("tokenizers/punkt")
except LookupError:
    nltk.download("punkt")

# --- Helpers ---

def clean_text(text):
    return text.lower().strip()


def is_valid_input(text):
    return len(text.strip()) >= 30 and re.search(r"[a-zA-Z]{3,}", text)


def fix_summary_capitalization(text):
    return ". ".join(sentence.strip().capitalize() for sentence in text.split(".") if sentence).strip() + "."


# --- Load IndoBERT ---
@st.cache_resource
def load_model():
    model_path = "models/indobert-fold0/checkpoint-3800"  # Ganti dengan path model terbaik kamu
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    model = AutoModelForSequenceClassification.from_pretrained(model_path)
    model.eval()
    return tokenizer, model


tokenizer, model = load_model()

# --- Load LexRank Summarizer (offline) ---
@st.cache_resource
def get_summarizer():
    """Return a cached instance of LexRankSummarizer."""
    return LexRankSummarizer()

summarizer = get_summarizer()

def summarize_text_indo(text, sentences_count: int = 3):
    """Ringkas teks Bahasa Indonesia menggunakan LexRank."""
    parser = PlaintextParser.from_string(text, SumyTokenizer("english"))  # Pakai English karena Indonesian tidak tersedia
    summary_sentences = summarizer(parser.document, sentences_count=sentences_count)
    summary = " ".join(str(sentence) for sentence in summary_sentences)
    return fix_summary_capitalization(summary) if summary else ""

# --- UI Header ---
st.markdown("<h1 style='text-align: center;'>Deteksi Berita Hoaks ğŸ”</h1>", unsafe_allow_html=True)

# --- UI Styling ---
st.markdown(
    """
    <style>
    .custom-box {
        background-color: #ffffff;
        border: 2px solid #b52f2f;
        padding: 25px;
        border-radius: 15px;
        box-shadow: 0 4px 14px rgba(0, 0, 0, 0.1);
        margin-top: 20px;
        margin-bottom: 30px;
    }
    .stTextArea textarea {
        background-color: #f9f9f9 !important;
        border: 1px solid #ccc !important;
        border-radius: 10px !important;
        padding: 15px !important;
        font-size: 16px !important;
    }
    .stButton > button {
        background-color: #b52f2f;
        color: white;
        font-weight: bold;
        font-size: 16px;
        border-radius: 8px;
        padding: 0.6em 1.5em;
        margin-top: 10px;
        border: none;
        transition: all 0.3s ease;
    }
    .stButton > button:hover {
        background-color: #ebebeb;
        border: 2px solid #b52f2f;
        transform: scale(1.04);
    }
    </style>
""", unsafe_allow_html=True)

# --- Input Section ---
st.markdown("<div class='custom-box'><h3>ğŸ“° Masukkan Artikel Berita</h3>", unsafe_allow_html=True)

text = st.text_area(
    label="",
    height=200,
    placeholder="Petunjuk:\nMasukkan teks dari sumber online\nGunakan Bahasa Indonesia\nMinimal 30 karakter",
    label_visibility="collapsed"
)

submit = st.button("ğŸ” Periksa")
st.markdown("</div>", unsafe_allow_html=True)

# --- Predict ---
if submit:
    text = text.strip()
    if not text:
        st.warning("âš ï¸ Tolong masukkan teks terlebih dahulu.")
    elif not is_valid_input(text):
        st.warning("âš ï¸ Teks terlalu pendek atau tidak valid. Masukkan minimal 30 karakter yang bermakna.")
    elif detect(text) != "id":
        st.warning("âš ï¸ Artikel harus menggunakan Bahasa Indonesia.")
    else:
        cleaned = clean_text(text)
        inputs = tokenizer(cleaned, return_tensors="pt", truncation=True, padding=True, max_length=256)

        with torch.no_grad():
            outputs = model(**inputs)
            TEMPERATURE = 1.5
            logits = outputs.logits / TEMPERATURE
            probs = torch.nn.functional.softmax(logits, dim=1)
            pred = torch.argmax(probs, dim=1).item()
            confidence = probs[0][pred].item()

        def describe_confidence(score):
            if score > 0.95:
                return "Sangat Yakin"
            elif score > 0.85:
                return "Cukup Yakin"
            elif score > 0.6:
                return "Yakin"
            else:
                return "Kurang Yakin"

        st.markdown("## Hasil Deteksi")
        result_label = "ğŸš¨ **Hoax**" if pred == 1 else "âœ… **Valid**"
        st.success(f"Hasil Deteksi: {result_label}")
        st.markdown(f"**Tingkat Keyakinan:** {confidence:.2%} ({describe_confidence(confidence)})")

        if confidence < 0.60:
            st.warning("âš ï¸ Hasil deteksi kurang meyakinkan. Harap verifikasi ulang informasi ini.")

        # Ringkasan
        st.markdown("### Ringkasan Artikel:")
        try:
            summary_text = summarize_text_indo(cleaned)
            if summary_text.strip():
                st.info(summary_text)
            else:
                st.info("Teks terlalu pendek untuk diringkas secara otomatis.")
        except Exception as e:
            st.warning(f"Gagal membuat ringkasan: {e}")

# --- Footer ---
st.markdown("<div style='margin-top: 25px;'></div>", unsafe_allow_html=True)
st.markdown("""---""")

with st.expander("Tentang Aplikasi â„¹ï¸", expanded=False):
    st.markdown(
        """
        Aplikasi ini digunakan untuk mendeteksi apakah sebuah artikel mengandung informasi hoaks atau tidak berdasarkan teks yang dimasukkan.
        """
    )

st.markdown(
    """
    <hr style='border-top: 1px solid #bbb;'>
    <div style='text-align: center; font-size: 14px;'>
        Dibuat oleh <b>Mohammad Ramadhan Zainoor</b> Â· Â© 2025 Â· <a href="https://github.com/zainoor" target="_blank">GitHub Repo</a>
    </div>
    """,
    unsafe_allow_html=True,
)
